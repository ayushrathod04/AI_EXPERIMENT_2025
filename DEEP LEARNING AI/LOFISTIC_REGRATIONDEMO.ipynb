{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils.py\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "\n",
        "def process_tweet(tweet):\n",
        "    \"\"\"Process tweet function.\n",
        "    Input:\n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "\n",
        "    \"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and  # remove stopwords\n",
        "                word not in string.punctuation):  # remove punctuation\n",
        "            # tweets_clean.append(word)\n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "\n",
        "    return tweets_clean\n",
        "\n",
        "\n",
        "def build_freqs(tweets, ys):\n",
        "    \"\"\"Build frequencies.\n",
        "    Input:\n",
        "        tweets: a list of tweets\n",
        "        ys: an m x 1 array with the sentiment label of each tweet\n",
        "            (either 0 or 1)\n",
        "    Output:\n",
        "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
        "        frequency\n",
        "    \"\"\"\n",
        "    # Convert np array to list since zip needs an iterable.\n",
        "    # The squeeze is necessary or the list ends up with one element.\n",
        "    # Also note that this is just a NOP if ys is already a list.\n",
        "    yslist = np.squeeze(ys).tolist()\n",
        "\n",
        "    # Start with an empty dictionary and populate it by looping over all tweets\n",
        "    # and over all processed words in each tweet.\n",
        "    freqs = {}\n",
        "    for y, tweet in zip(yslist, tweets):\n",
        "        for word in process_tweet(tweet):\n",
        "            pair = (word, y)\n",
        "            if pair in freqs:\n",
        "                freqs[pair] += 1\n",
        "            else:\n",
        "                freqs[pair] = 1\n",
        "\n",
        "    return freqs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yY7b43-gofs",
        "outputId": "5e406365-3c25-4128-eaa1-1cabe8ee969e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "from os import getcwd\n",
        "# import w1_unittest\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEuW8WGziECy",
        "outputId": "5ea27fda-9b81-40a0-e0a0-7e7733fea1d6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filePath = f\"{getcwd()}/../tmp2/\"\n",
        "nltk.data.path.append(filePath)\n"
      ],
      "metadata": {
        "id": "xegEiK32iLz9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select the set of positive and negative tweets\n",
        "from nltk.corpus import twitter_samples\n",
        "\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ],
      "metadata": {
        "id": "P6K9PbOvivW6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into two pieces, one for training and one for testing (validation set)\n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg"
      ],
      "metadata": {
        "id": "e4BxA4zMiyhz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the utility functions\n",
        "from utils import process_tweet, build_freqs\n",
        "import numpy as np\n",
        "\n",
        "# Process tweets\n",
        "all_tweets = all_positive_tweets + all_negative_tweets\n",
        "ys = np.append(np.ones((len(all_positive_tweets), 1)), np.zeros((len(all_negative_tweets), 1)), axis=0)\n",
        "\n",
        "# Note: process_tweet is called inside build_freqs, so we don't need to pre-process here.\n",
        "# processed_tweets = [process_tweet(tweet) for tweet in all_tweets]\n",
        "\n",
        "# Build frequency dictionary\n",
        "freqs = build_freqs(all_tweets, ys)\n",
        "\n",
        "print(\"Frequency dictionary built successfully!\")\n",
        "print(f\"Size of frequency dictionary: {len(freqs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1p_PU0Wjm6J",
        "outputId": "89b8a04b-d533-4be0-f739-46046a4004a7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency dictionary built successfully!\n",
            "Size of frequency dictionary: 13141\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create label arrays for training and testing data\n",
        "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)\n",
        "\n",
        "print(\"train_y.shape: \", train_y.shape)\n",
        "print(\"test_y.shape: \", test_y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-0zOErEjrIZ",
        "outputId": "1d8a70aa-3d99-4bc8-f299-2bfa25c11c47"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_y.shape:  (8000, 1)\n",
            "test_y.shape:  (2000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create frequency dictionary\n",
        "freqs = build_freqs(train_x, train_y)\n",
        "\n",
        "# check the output\n",
        "print(\"type(freqs) = \" + str(type(freqs)))\n",
        "print(\"len(freqs) = \" + str(len(freqs.keys())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x90_dHogjvcP",
        "outputId": "5931d255-d7c0-413a-8d9a-4270e12c9eb3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type(freqs) = <class 'dict'>\n",
            "len(freqs) = 11397\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test the function below\n",
        "print('This is an example of a positive tweet: \\n', train_x[0])\n",
        "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfjR-TyskV7C",
        "outputId": "ec9b09a4-31f0-45ed-b066-6fb44b741d58"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is an example of a positive tweet: \n",
            " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "\n",
            "This is an example of the processed version of the tweet: \n",
            " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    '''\n",
        "    Input:\n",
        "        z: is the input (can be a scalar or an array)\n",
        "    Output:\n",
        "        h: the sigmoid of z\n",
        "    '''\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # calculate the sigmoid of z\n",
        "    h = 1 / (1 + np.exp(-z))\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return h"
      ],
      "metadata": {
        "id": "jG0jHScVkdff"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing your function\n",
        "if (sigmoid(0) == 0.5):\n",
        "    print('SUCCESS!')\n",
        "else:\n",
        "    print('Oops!')\n",
        "\n",
        "if (sigmoid(4.92) == 0.9927537604041685):\n",
        "    print('CORRECT!')\n",
        "else:\n",
        "    print('Oops again!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IWBDLiakmRj",
        "outputId": "1c6b2c18-c161-48a8-cfb6-07909a34a47f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SUCCESS!\n",
            "CORRECT!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Manual tests\n",
        "def test_sigmoid():\n",
        "    assert np.isclose(sigmoid(0), 0.5), \"❌ Test failed for input 0\"\n",
        "    assert np.isclose(sigmoid(2), 1 / (1 + np.exp(-2))), \"❌ Test failed for input 2\"\n",
        "    assert np.isclose(sigmoid(-2), 1 / (1 + np.exp(2))), \"❌ Test failed for input -2\"\n",
        "    print(\"✅ All sigmoid tests passed!\")\n",
        "\n",
        "test_sigmoid()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXDq-lHOkpo8",
        "outputId": "4e86f34a-b07e-4ffb-c56f-f015edf2e89d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All sigmoid tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "-1 * (1 - 0) * np.log(1 - 0.9999) # loss is about 9.2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Eh32-D3ktPr",
        "outputId": "3dd1242b-1c86-42b6-b095-cfcabe64891e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(9.210340371976294)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C2 GRADED FUNCTION: gradientDescent\n",
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "    '''\n",
        "    Input:\n",
        "        x: matrix of features which is (m,n+1)\n",
        "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
        "        theta: weight vector of dimension (n+1,1)\n",
        "        alpha: learning rate\n",
        "        num_iters: number of iterations you want to train your model for\n",
        "    Output:\n",
        "        J: the final cost\n",
        "        theta: your final weight vector\n",
        "    Hint: you might want to print the cost to make sure that it is going down.\n",
        "    '''\n",
        "    ### START CODE HERE ###\n",
        "    # get 'm', the number of rows in matrix x\n",
        "    m = x.shape[0]\n",
        "    for i in range(0, num_iters):\n",
        "\n",
        "        # get z, the dot product of x and theta\n",
        "        z = np.dot(x,theta)\n",
        "\n",
        "        # get the sigmoid of h\n",
        "        h = sigmoid(z)\n",
        "\n",
        "        # calculate the cost function\n",
        "        J = -1./m * (np.dot(y.transpose(), np.log(h)) + np.dot((1-y).transpose(),np.log(1-h)))\n",
        "\n",
        "        # update the weights theta\n",
        "        theta = theta - (alpha/m) * np.dot(x.transpose(),(h-y))\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    J = float(J)\n",
        "    return J, theta"
      ],
      "metadata": {
        "id": "__lTHPuIljIK"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the function\n",
        "# Construct a synthetic test case using numpy PRNG functions\n",
        "np.random.seed(1)\n",
        "# X input is 10 x 3 with ones for the bias terms\n",
        "tmp_X = np.append(np.ones((10, 1)), np.random.rand(10, 2) * 2000, axis=1)\n",
        "# Y Labels are 10 x 1\n",
        "tmp_Y = (np.random.rand(10, 1) > 0.35).astype(float)\n",
        "\n",
        "# Apply gradient descent\n",
        "tmp_J, tmp_theta = gradientDescent(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 700)\n",
        "print(f\"The cost after training is {tmp_J:.8f}.\")\n",
        "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(tmp_theta)]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyedLXpklo3g",
        "outputId": "8e76c496-b91a-4315-9e0d-7401b1ba1e89"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cost after training is 0.67094970.\n",
            "The resulting vector of weights is [np.float64(4.1e-07), np.float64(0.00035658), np.float64(7.309e-05)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-27-4278095536.py:33: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  J = float(J)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradientDescent(x, y, theta, alpha, num_iters, computeCost, computeGradient):\n",
        "    for i in range(num_iters):\n",
        "        grad = computeGradient(x, y, theta)\n",
        "        theta = theta - alpha * grad\n",
        "    return theta\n"
      ],
      "metadata": {
        "id": "H2w3-h61luTA"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C3 GRADED FUNCTION: extract_features\n",
        "def extract_features(tweet, freqs, process_tweet=process_tweet):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a list of words for one tweet\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "    Output:\n",
        "        x: a feature vector of dimension (1,3)\n",
        "    '''\n",
        "    # process_tweet tokenizes, stems, and removes stopwords\n",
        "    word_l = process_tweet(tweet)\n",
        "\n",
        "    # 3 elements in the form of a 1 x 3 vector\n",
        "    x = np.zeros((1, 3))\n",
        "\n",
        "    #bias term is set to 1\n",
        "    x[0,0] = 1\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # loop through each word in the list of words\n",
        "    # loop through each word in the list of words\n",
        "    for word in word_l:\n",
        "\n",
        "        # increment the word count for the positive label 1\n",
        "        x[0,1] += freqs.get((word, 1.0),0)\n",
        "\n",
        "        # increment the word count for the negative label 0\n",
        "        x[0,2] += freqs.get((word, 0.0),0)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    assert(x.shape == (1, 3))\n",
        "    return x"
      ],
      "metadata": {
        "id": "hWD3KPhUmboj"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp1 = extract_features(train_x[0], freqs)\n",
        "print(tmp1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuxGrvPGnbjc",
        "outputId": "c18c6f7e-d9e8-475d-f22e-f57bef0848ff"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.000e+00 3.133e+03 6.100e+01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check for when the words are not in the freqs dictionary\n",
        "tmp2 = extract_features('blorb bleeeeb bloooob', freqs)\n",
        "print(tmp2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWhX5vDUnfV3",
        "outputId": "74a42eeb-c959-49e1-cdd7-0d6ae873e089"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(tweet, freqs):\n",
        "    word_l = tweet.lower().split()\n",
        "    x = np.zeros((3,))\n",
        "    x[0] = 1  # bias term\n",
        "    for word in word_l:\n",
        "        x[1] += freqs.get((word, 1), 0)\n",
        "        x[2] += freqs.get((word, 0), 0)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "Bxyyhuu3nyxT"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C2 GRADED FUNCTION: gradientDescent\n",
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "    '''\n",
        "    Input:\n",
        "        x: matrix of features which is (m,n+1)\n",
        "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
        "        theta: weight vector of dimension (n+1,1)\n",
        "        alpha: learning rate\n",
        "        num_iters: number of iterations you want to train your model for\n",
        "    Output:\n",
        "        J: the final cost\n",
        "        theta: your final weight vector\n",
        "    Hint: you might want to print the cost to make sure that it is going down.\n",
        "    '''\n",
        "    ### START CODE HERE ###\n",
        "    # get 'm', the number of rows in matrix x\n",
        "    m = x.shape[0]\n",
        "    for i in range(0, num_iters):\n",
        "\n",
        "        # get z, the dot product of x and theta\n",
        "        z = np.dot(x,theta)\n",
        "\n",
        "        # get the sigmoid of h\n",
        "        h = sigmoid(z)\n",
        "\n",
        "        # calculate the cost function\n",
        "        J = -1./m * (np.dot(y.transpose(), np.log(h)) + np.dot((1-y).transpose(),np.log(1-h)))\n",
        "\n",
        "        # update the weights theta\n",
        "        theta = theta - (alpha/m) * np.dot(x.transpose(),(h-y))\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    J = float(J)\n",
        "    return J, theta\n",
        "\n",
        "\n",
        "# collect the features 'x' and stack them into a matrix 'X'\n",
        "X = np.zeros((len(train_x), 3))\n",
        "for i in range(len(train_x)):\n",
        "    X[i, :]= extract_features(train_x[i], freqs)\n",
        "\n",
        "# training labels corresponding to X\n",
        "Y = train_y\n",
        "\n",
        "# Apply gradient descent\n",
        "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
        "print(f\"The cost after training is {J:.8f}.\")\n",
        "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaok6xzHof7j",
        "outputId": "50b1c438-9c2a-426d-d545-83749158da70"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cost after training is 0.30801877.\n",
            "The resulting vector of weights is [np.float64(4e-08), np.float64(0.00049776), np.float64(-0.00051491)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-46-4278605255.py:33: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  J = float(J)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C4 GRADED FUNCTION: predict_tweet\n",
        "def predict_tweet(tweet, freqs, theta):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a string\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "        theta: (3,1) vector of weights\n",
        "    Output:\n",
        "        y_pred: the probability of a tweet being positive or negative\n",
        "    '''\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # extract the features of the tweet and store it into x\n",
        "    x = extract_features(tweet,freqs)\n",
        "\n",
        "    # make the prediction using x and theta\n",
        "    y_pred = sigmoid(np.dot(x,theta))\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "2t1b6hWzoqub"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to test your function\n",
        "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
        "    print( '%s -> %f' % (tweet, predict_tweet(tweet, freqs, theta)))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPZccZVdozTF",
        "outputId": "fdb357ef-3c20-4e89-ff0c-1dee6f18babb"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am happy -> 0.500000\n",
            "I am bad -> 0.494791\n",
            "this movie should have been great. -> 0.499996\n",
            "great -> 0.514864\n",
            "great great -> 0.529702\n",
            "great great great -> 0.544487\n",
            "great great great great -> 0.559195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-48-1003351.py:3: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print( '%s -> %f' % (tweet, predict_tweet(tweet, freqs, theta)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to check the sentiment of your own tweet below\n",
        "my_tweet = 'I am learning :)'\n",
        "predict_tweet(my_tweet, freqs, theta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rt5t9Itto3sf",
        "outputId": "1c7dec87-2bc4-4396-f7e1-39b6302cc546"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.81341154])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your function\n",
        "w1_unittest=test_predict_tweet=(predict_tweet, freqs, theta)"
      ],
      "metadata": {
        "id": "xU0EIWD-o9ig"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_logistic_regression(test_x, test_y, freqs, theta, predict_tweet=predict_tweet):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        test_x: a list of tweets\n",
        "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
        "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
        "        theta: weight vector of dimension (3, 1)\n",
        "    Output:\n",
        "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # the list for storing predictions\n",
        "    y_hat = []\n",
        "\n",
        "    for tweet in test_x:\n",
        "        # get the label prediction for the tweet\n",
        "        y_pred = predict_tweet(tweet, freqs, theta)\n",
        "        if y_pred > 0.5:\n",
        "            # append 1.0 to the list\n",
        "            y_hat.append(1)\n",
        "        else:\n",
        "            # append 0 to the list\n",
        "            y_hat.append(0)\n",
        "\n",
        "    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n",
        "    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
        "\n",
        "    accuracy = (y_hat==np.squeeze(test_y)).sum()/len(test_x)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "aRCNjM0WpCQl"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
        "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXhb-Dkhp54g",
        "outputId": "afa18c05-205d-4047-fd84-75c8146e79df"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression model's accuracy = 0.9515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Some error analysis done for you\n",
        "print('Label Predicted Tweet')\n",
        "for x,y in zip(test_x,test_y):\n",
        "    y_hat = predict_tweet(x, freqs, theta)\n",
        "\n",
        "    if np.abs(y - (y_hat > 0.5)) > 0:\n",
        "        print('THE TWEET IS:', x)\n",
        "        print('THE PROCESSED TWEET IS:', process_tweet(x))\n",
        "        print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhFbx0ZYp-Yk",
        "outputId": "346e1fad-5fe6-4a84-f34e-d9157c363bdb"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Predicted Tweet\n",
            "THE TWEET IS: @SasaRichardson @Stefbystef_ @Frgt10_Anthem HAHAHAHAHAHAHAHAHAHAHAHAHAHA I AM DYING :)) I AM LITERALLY IN FRONT OF IT AND I WAS LIKE\n",
            "THE PROCESSED TWEET IS: ['hahahahahahahahahahahahahaha', 'die', ':)', 'liter', 'front', 'like']\n",
            "1\t0.49944285\tb'hahahahahahahahahahahahahaha die :) liter front like'\n",
            "THE TWEET IS: All my Bae :)) &lt;3 \n",
            "By : @Merima_Beslagic http://t.co/gDy1trnfjV\n",
            "THE PROCESSED TWEET IS: ['bae', ':)', '<3']\n",
            "1\t0.49972112\tb'bae :) <3'\n",
            "THE TWEET IS: I :)\"@Kreizi_: Who wants a #FF S/O??\"\n",
            "THE PROCESSED TWEET IS: [':)', 'want', 'ff']\n",
            "1\t0.49974256\tb':) want ff'\n",
            "THE TWEET IS: @CaballeroSerena you actually need to stop tweeting and driving! :-)))))))\n",
            "THE PROCESSED TWEET IS: ['actual', 'need', 'stop', 'tweet', 'drive', ':-)']\n",
            "1\t0.49400619\tb'actual need stop tweet drive :-)'\n",
            "THE TWEET IS: @Madison420Ivy  \"We\" ??? you wish :D:D:D:D:D:D:D kidding\n",
            "THE PROCESSED TWEET IS: ['wish', ':d', ':d', ':d', ':d', ':d', ':d', ':d', 'kid']\n",
            "1\t0.49562352\tb'wish :d :d :d :d :d :d :d kid'\n",
            "THE TWEET IS: @MarkBreech Not sure it would be good thing 4 my bottom daring 2 say 2 Miss B but Im gonna be so stubborn on mouth soaping ! #NotHavingit :p\n",
            "THE PROCESSED TWEET IS: ['sure', 'would', 'good', 'thing', '4', 'bottom', 'dare', '2', 'say', '2', 'miss', 'b', 'im', 'gonna', 'stubborn', 'mouth', 'soap', 'nothavingit', ':p']\n",
            "1\t0.48986438\tb'sure would good thing 4 bottom dare 2 say 2 miss b im gonna stubborn mouth soap nothavingit :p'\n",
            "THE TWEET IS: It's not like it hurt my feelings or anything right?? :))))))))))))\n",
            "THE PROCESSED TWEET IS: ['like', 'hurt', 'feel', 'anyth', 'right', ':)']\n",
            "1\t0.49609175\tb'like hurt feel anyth right :)'\n",
            "THE TWEET IS: Whatsapp with roommate\n",
            "\n",
            "\"Do you want anything from Paris?\"\n",
            "\"A French man :)\"\n",
            "\n",
            "LOL sure I'll just head to the park and grab one.\n",
            "THE PROCESSED TWEET IS: ['whatsapp', 'roommat', 'want', 'anyth', 'pari', 'french', 'man', ':)', 'lol', 'sure', 'head', 'park', 'grab', 'one']\n",
            "1\t0.48680068\tb'whatsapp roommat want anyth pari french man :) lol sure head park grab one'\n",
            "THE TWEET IS: @charliehaarding @WeAlIlKnowA @ElliotPender @J_Mezzer smiling after he received a text from his dad saying suck him off :))\n",
            "THE PROCESSED TWEET IS: ['smile', 'receiv', 'text', 'dad', 'say', 'suck', ':)']\n",
            "1\t0.49758005\tb'smile receiv text dad say suck :)'\n",
            "THE TWEET IS: @SasaRichardson @Stefbystef_ @Frgt10_Anthem sombrero high?? OKAY I'M LOST NOW :)) http://t.co/2VDPgb5EpK\n",
            "THE PROCESSED TWEET IS: ['sombrero', 'high', 'okay', 'lost', ':)']\n",
            "1\t0.49767876\tb'sombrero high okay lost :)'\n",
            "THE TWEET IS: @mariaburtea @5SOS_Nightly wow you really hate me :))))\n",
            "THE PROCESSED TWEET IS: ['wow', 'realli', 'hate', ':)']\n",
            "1\t0.49667899\tb'wow realli hate :)'\n",
            "THE TWEET IS: @heyoppar @Zain9898 @bemybelief @hetthuocchua there's gonna be another one in the finale :)))))\n",
            "THE PROCESSED TWEET IS: [\"there'\", 'gonna', 'anoth', 'one', 'final', ':)']\n",
            "1\t0.49258198\tb\"there' gonna anoth one final :)\"\n",
            "THE TWEET IS: @Anyi_anonadada @JosPastr @norcoreano a tope :))\n",
            "THE PROCESSED TWEET IS: ['tope', ':)']\n",
            "1\t0.49987128\tb'tope :)'\n",
            "THE TWEET IS: and I have to be up for 7 :):):):)\n",
            "THE PROCESSED TWEET IS: ['7', ':)', ':)', ':)', ':)']\n",
            "1\t0.49856258\tb'7 :) :) :) :)'\n",
            "THE TWEET IS: @mariammaslouhi If it HAD been two ~17 year olds deeply in love? :-)))\n",
            "THE PROCESSED TWEET IS: ['two', '17', 'year', 'old', 'deepli', 'love', ':-)']\n",
            "1\t0.49953676\tb'two 17 year old deepli love :-)'\n",
            "THE TWEET IS: @wontanim yep we're all trash af   im from indonesia hbu? :-))\n",
            "THE PROCESSED TWEET IS: ['yep', 'trash', 'af', 'im', 'indonesia', 'hbu', ':-)']\n",
            "1\t0.49099889\tb'yep trash af im indonesia hbu :-)'\n",
            "THE TWEET IS: Crazy girlfriends be like :-)(-: Jesus Christ. http://t.co/RTWjc7e1lM\n",
            "THE PROCESSED TWEET IS: ['crazi', 'girlfriend', 'like', ':-)', '(-:', 'jesu', 'christ']\n",
            "1\t0.49983761\tb'crazi girlfriend like :-) (-: jesu christ'\n",
            "THE TWEET IS: How you? ☺ \"@B_Indiie: @Tumeylo morning babe! :)\"\n",
            "THE PROCESSED TWEET IS: ['☺', 'morn', 'babe', ':)']\n",
            "1\t0.49999143\tb' morn babe :)'\n",
            "THE TWEET IS: @dualityknight @bravefrontiergl lol sorry /-\\ ultor is my very first hitter so...i favor him too much :))\n",
            "THE PROCESSED TWEET IS: ['lol', 'sorri', 'ultor', 'first', 'hitter', '...', 'favor', 'much', ':)']\n",
            "1\t0.49877768\tb'lol sorri ultor first hitter ... favor much :)'\n",
            "THE TWEET IS: Only my bad would remind me to exercise at 1:12 in the am :-). \n",
            " I miss her. She needs to come back. Now.\n",
            "THE PROCESSED TWEET IS: ['bad', 'would', 'remind', 'exercis', '1:12', ':-)', 'miss', 'need', 'come', 'back']\n",
            "1\t0.46637649\tb'bad would remind exercis 1:12 :-) miss need come back'\n",
            "THE TWEET IS: At least there's one cool thing :-))))\n",
            "THE PROCESSED TWEET IS: ['least', \"there'\", 'one', 'cool', 'thing', ':-)']\n",
            "1\t0.49985904\tb\"least there' one cool thing :-)\"\n",
            "THE TWEET IS: 20 ghanton se light nahi :))))))\n",
            "THE PROCESSED TWEET IS: ['20', 'ghanton', 'se', 'light', 'nahi', ':)']\n",
            "1\t0.49955809\tb'20 ghanton se light nahi :)'\n",
            "THE TWEET IS: @pakalupapito imma use this next time :)))\n",
            "THE PROCESSED TWEET IS: ['imma', 'use', 'next', 'time', ':)']\n",
            "1\t0.49682538\tb'imma use next time :)'\n",
            "THE TWEET IS: @rinpunzel_ what are you promoting?:)\n",
            "THE PROCESSED TWEET IS: ['promot', ':)']\n",
            "1\t0.49959668\tb'promot :)'\n",
            "THE TWEET IS: Just need to get through work today then to Ibiza! :):):) #birthdaymoneyforjesusjuice\n",
            "THE PROCESSED TWEET IS: ['need', 'get', 'work', 'today', 'ibiza', ':)', ':)', ':)', 'birthdaymoneyforjesusjuic']\n",
            "1\t0.49181513\tb'need get work today ibiza :) :) :) birthdaymoneyforjesusjuic'\n",
            "THE TWEET IS: I WANT to create the FIRST #Bboying #Cardgame with #pixelart #gamedesign :D!!!! What do you think ?? #indiedev #pixel_dailies #gamedev\n",
            "THE PROCESSED TWEET IS: ['want', 'creat', 'first', 'bboy', 'cardgam', 'pixelart', 'gamedesign', ':d', 'think', 'indiedev', 'pixel_daili', 'gamedev']\n",
            "1\t0.48537283\tb'want creat first bboy cardgam pixelart gamedesign :d think indiedev pixel_daili gamedev'\n",
            "THE TWEET IS: Those who move forward with a happy spirit will find that things always work out \\:)/\n",
            "THE PROCESSED TWEET IS: ['move', 'forward', 'happi', 'spirit', 'find', 'thing', 'alway', 'work', '\\\\:']\n",
            "1\t0.49625895\tb'move forward happi spirit find thing alway work \\\\:'\n",
            "THE TWEET IS: @iamAhmadshahzad aameen..:)\n",
            "Long Live Pakistan..#BleedGreen..✌🏻️\n",
            "THE PROCESSED TWEET IS: ['aameen', '..', ':)', 'long', 'live', 'pakistan', '..', 'bleedgreen', '..', '✌🏻', '️']\n",
            "1\t0.49772603\tb'aameen .. :) long live pakistan .. bleedgreen ..  '\n",
            "THE TWEET IS: getting home at 4 waking up at 9 :):)\n",
            "THE PROCESSED TWEET IS: ['get', 'home', '4', 'wake', '9', ':)', ':)']\n",
            "1\t0.49594536\tb'get home 4 wake 9 :) :)'\n",
            "THE TWEET IS: off to the park to get some sunlight : )\n",
            "THE PROCESSED TWEET IS: ['park', 'get', 'sunlight']\n",
            "1\t0.49670523\tb'park get sunlight'\n",
            "THE TWEET IS: Always be positive :). #postive #selfie https://t.co/FDFpAD3wzd\n",
            "THE PROCESSED TWEET IS: ['alway', 'posit', ':)', 'postiv', 'selfi']\n",
            "1\t0.49999572\tb'alway posit :) postiv selfi'\n",
            "THE TWEET IS: Oh lovely lovelayyy! Thanks! It is ok about the kidney.. I don't want it anyway. :p https://t.co/ZbO0cjqhzG\n",
            "THE PROCESSED TWEET IS: ['oh', 'love', 'lovelayyy', 'thank', 'ok', 'kidney', '..', 'want', 'anyway', ':p']\n",
            "1\t0.49391240\tb'oh love lovelayyy thank ok kidney .. want anyway :p'\n",
            "THE TWEET IS: @msarosh Uff Itna Miss karhy thy ap :p\n",
            "THE PROCESSED TWEET IS: ['uff', 'itna', 'miss', 'karhi', 'thi', 'ap', ':p']\n",
            "1\t0.48390657\tb'uff itna miss karhi thi ap :p'\n",
            "THE TWEET IS: making Alyssa rub my tummy :)))\n",
            "THE PROCESSED TWEET IS: ['make', 'alyssa', 'rub', 'tummi', ':)']\n",
            "1\t0.49987128\tb'make alyssa rub tummi :)'\n",
            "THE TWEET IS: \"You make me alive, You make me suffer, You make me feel.. \n",
            "\n",
            "Addictive song i always sing in KARAOKE :-)..\n",
            "THE PROCESSED TWEET IS: ['make', 'aliv', 'make', 'suffer', 'make', 'feel', '..', 'addict', 'song', 'alway', 'sing', 'karaok', ':-)', '..']\n",
            "1\t0.49000436\tb'make aliv make suffer make feel .. addict song alway sing karaok :-) ..'\n",
            "THE TWEET IS: @5SOSTumblrx I threw my phone at the wall :)))))\n",
            "THE PROCESSED TWEET IS: ['threw', 'phone', 'wall', ':)']\n",
            "1\t0.49863557\tb'threw phone wall :)'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-57-289099189.py:9: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE TWEET IS: Who's still awake ?:)))\n",
            "THE PROCESSED TWEET IS: [\"who'\", 'still', 'awak', ':)']\n",
            "1\t0.49121759\tb\"who' still awak :)\"\n",
            "THE TWEET IS: @adoringpreston someone unfaved :((\n",
            "THE PROCESSED TWEET IS: ['someon', 'unfav', ':(']\n",
            "0\t0.50000001\tb'someon unfav :('\n",
            "THE TWEET IS: @Quality_CE don't gas me :((\n",
            "THE PROCESSED TWEET IS: ['ga', ':(']\n",
            "0\t0.50000001\tb'ga :('\n",
            "THE TWEET IS: as if not an apology isn't available in australia :(( @BeaMiller\n",
            "THE PROCESSED TWEET IS: ['apolog', 'avail', 'australia', ':(']\n",
            "0\t0.50010730\tb'apolog avail australia :('\n",
            "THE TWEET IS: 😂😂 what a night :(((((( so devo xxx https://t.co/9ixTnyBXLb\n",
            "THE PROCESSED TWEET IS: ['😂', '😂', 'night', ':(', 'devo', 'xxx']\n",
            "0\t0.50252757\tb'  night :( devo xxx'\n",
            "THE TWEET IS: @kingsxcreed daaaaaamn :((\n",
            "THE PROCESSED TWEET IS: ['daaamn', ':(']\n",
            "0\t0.50000001\tb'daaamn :('\n",
            "THE TWEET IS: Dadas uh iphone na, :(:( [pic] — https://t.co/Jr4U98A8ja\n",
            "THE PROCESSED TWEET IS: ['dada', 'uh', 'iphon', 'na', ':(', ':(', 'pic', '—']\n",
            "0\t0.50016314\tb'dada uh iphon na :( :( pic '\n",
            "THE TWEET IS: @ssulstagram why you unfollow me unnie?:(\n",
            "THE PROCESSED TWEET IS: ['unfollow', 'unni', ':(']\n",
            "0\t0.50568967\tb'unfollow unni :('\n",
            "THE TWEET IS: remember was #2 on gaon and bad was #6 but infnt are eligible and apink are not :(((( acube why'd u do this\n",
            "THE PROCESSED TWEET IS: ['rememb', '2', 'gaon', 'bad', '6', 'infnt', 'elig', 'apink', ':(', 'acub', \"why'd\", 'u']\n",
            "0\t0.50047706\tb\"rememb 2 gaon bad 6 infnt elig apink :( acub why'd u\"\n",
            "THE TWEET IS: Bullshit......:((\n",
            "THE PROCESSED TWEET IS: ['bullshit', '...', ':(']\n",
            "0\t0.50000001\tb'bullshit ... :('\n",
            "THE TWEET IS: Srsly, Y U do that? :((  https://t.co/g0r01GGj2b\n",
            "THE PROCESSED TWEET IS: ['srsli', 'u', ':(']\n",
            "0\t0.50594761\tb'srsli u :('\n",
            "THE TWEET IS: @1994sdork omg :-(( I LOVE YOU SO MUCH MONICA SEE YOU SOON AAAHHH !!!\n",
            "THE PROCESSED TWEET IS: ['omg', ':-(', 'love', 'much', 'monica', 'see', 'soon', 'aaahhh']\n",
            "0\t0.52075385\tb'omg :-( love much monica see soon aaahhh'\n",
            "THE TWEET IS: :((( (at Crepes 40) — https://t.co/bazEmHlhYL\n",
            "THE PROCESSED TWEET IS: [':(', 'crepe', '40', '—']\n",
            "0\t0.50042059\tb':( crepe 40 '\n",
            "THE TWEET IS: OverPerhatian:(:(\n",
            "THE PROCESSED TWEET IS: ['overperhatian', ':(', ':(']\n",
            "0\t0.50000001\tb'overperhatian :( :('\n",
            "THE TWEET IS: @ayyedolans DO U HAVE GRAY'S FOLLOWK?!! OMG ITS TOO HARD these DAYS U KNOW :(:(\n",
            "THE PROCESSED TWEET IS: ['u', \"gray'\", 'followk', 'omg', 'hard', 'day', 'u', 'know', ':(', ':(']\n",
            "0\t0.50610328\tb\"u gray' followk omg hard day u know :( :(\"\n",
            "THE TWEET IS: I'm so hungry :((\n",
            "THE PROCESSED TWEET IS: ['hungri', ':(']\n",
            "0\t0.50000001\tb'hungri :('\n",
            "THE TWEET IS: :((((( matt\n",
            "THE PROCESSED TWEET IS: [':(', 'matt']\n",
            "0\t0.50049348\tb':( matt'\n",
            "THE TWEET IS: hey someone text me :((\n",
            "THE PROCESSED TWEET IS: ['hey', 'someon', 'text', ':(']\n",
            "0\t0.50471152\tb'hey someon text :('\n",
            "THE TWEET IS: @shagotpm ikr hais why would someone sell that precious thing? :((\n",
            "THE PROCESSED TWEET IS: ['ikr', 'hai', 'would', 'someon', 'sell', 'preciou', 'thing', ':(']\n",
            "0\t0.50119316\tb'ikr hai would someon sell preciou thing :('\n",
            "THE TWEET IS: @phenomyoutube u probs had more fun with david than me : (\n",
            "THE PROCESSED TWEET IS: ['u', 'prob', 'fun', 'david']\n",
            "0\t0.50909656\tb'u prob fun david'\n",
            "THE TWEET IS: Babyy :((( http://t.co/WX0gjCms9t\n",
            "THE PROCESSED TWEET IS: ['babyy', ':(']\n",
            "0\t0.50000001\tb'babyy :('\n",
            "THE TWEET IS: My last day in Indiana :((((\n",
            "THE PROCESSED TWEET IS: ['last', 'day', 'indiana', ':(']\n",
            "0\t0.50649683\tb'last day indiana :('\n",
            "THE TWEET IS: Missin my homeslice on her bday :(( &lt;/333 @wraithmedia http://t.co/1J3qYUTTqG\n",
            "THE PROCESSED TWEET IS: ['missin', 'homeslic', 'bday', ':(', '</3', '33']\n",
            "0\t0.50048062\tb'missin homeslic bday :( </3 33'\n",
            "THE TWEET IS: MY FAV EMOTICON RIGHT NOW IS THE \" :(: \" EMOTICON\n",
            "THE PROCESSED TWEET IS: ['fav', 'emoticon', 'right', ':(', 'emoticon']\n",
            "0\t0.50081974\tb'fav emoticon right :( emoticon'\n",
            "THE TWEET IS: @iperfectyonce @justinbieber its my biggest dream can u follow me brooo :((((\n",
            "THE PROCESSED TWEET IS: ['biggest', 'dream', 'u', 'follow', 'brooo', ':(']\n",
            "0\t0.52209011\tb'biggest dream u follow brooo :('\n",
            "THE TWEET IS: @swcgtbh leave me alone :(((\n",
            "THE PROCESSED TWEET IS: ['leav', 'alon', ':(']\n",
            "0\t0.50000001\tb'leav alon :('\n",
            "THE TWEET IS: why isn't anyone awake :-((\n",
            "THE PROCESSED TWEET IS: ['anyon', 'awak', ':-(']\n",
            "0\t0.50000001\tb'anyon awak :-('\n",
            "THE TWEET IS: @wajiyaamjad ok I'll write u every week :((\n",
            "THE PROCESSED TWEET IS: ['ok', 'write', 'u', 'everi', 'week', ':(']\n",
            "0\t0.50958578\tb'ok write u everi week :('\n",
            "THE TWEET IS: @fyoudontknow WAIT ITS GONE. LITERALLY IT IS :(((((\n",
            "THE PROCESSED TWEET IS: ['wait', 'gone', 'liter', ':(']\n",
            "0\t0.50220999\tb'wait gone liter :('\n",
            "THE TWEET IS: @BRATSUNITED @ShutUpPenguin @epiphanichood why did it? Lmao. I only back out if it gets really really serious....:((\n",
            "THE PROCESSED TWEET IS: ['lmao', 'back', 'get', 'realli', 'realli', 'seriou', '...', ':(']\n",
            "0\t0.50468172\tb'lmao back get realli realli seriou ... :('\n",
            "THE TWEET IS: @cloudljp same :((((( i hate school U DONT UNDERSTAND i missed a LOT i blame school\n",
            "THE PROCESSED TWEET IS: [':(', 'hate', 'school', 'u', 'dont', 'understand', 'miss', 'lot', 'blame', 'school']\n",
            "0\t0.50103525\tb':( hate school u dont understand miss lot blame school'\n",
            "THE TWEET IS: #ZaynIsComingBackOnJuly26 this is not funny :(((((\n",
            "THE PROCESSED TWEET IS: ['zayniscomingbackonjuli', '26', 'funni', ':(']\n",
            "0\t0.50000001\tb'zayniscomingbackonjuli 26 funni :('\n",
            "THE TWEET IS: pats jay : (\n",
            "THE PROCESSED TWEET IS: ['pat', 'jay']\n",
            "0\t0.50037333\tb'pat jay'\n",
            "THE TWEET IS: @JayMcGuiness please baby follow me :(((\n",
            "THE PROCESSED TWEET IS: ['pleas', 'babi', 'follow', ':(']\n",
            "0\t0.51623665\tb'pleas babi follow :('\n",
            "THE TWEET IS: !! NUGGETS AND FRIES !! :-((\n",
            "THE PROCESSED TWEET IS: ['nugget', 'fri', ':-(']\n",
            "0\t0.50000001\tb'nugget fri :-('\n",
            "THE TWEET IS: @Wufanited tomorrow :(((\n",
            "THE PROCESSED TWEET IS: ['tomorrow', ':(']\n",
            "0\t0.50051073\tb'tomorrow :('\n",
            "THE TWEET IS: @TotallyWonwooed lol, i'm too lazy to learn ps :((((\n",
            "THE PROCESSED TWEET IS: ['lol', 'lazi', 'learn', 'ps', ':(']\n",
            "0\t0.50010301\tb'lol lazi learn ps :('\n",
            "THE TWEET IS: don't sleep. I'm here : (\n",
            "THE PROCESSED TWEET IS: ['sleep']\n",
            "0\t0.50000001\tb'sleep'\n",
            "THE TWEET IS: :((( no they killed him\n",
            "THE PROCESSED TWEET IS: [':(', 'kill']\n",
            "0\t0.50000001\tb':( kill'\n",
            "THE TWEET IS: @jhapaliMailo , some people :((\n",
            "How r  u?\n",
            "THE PROCESSED TWEET IS: ['peopl', ':(', 'r', 'u']\n",
            "0\t0.50020173\tb'peopl :( r u'\n",
            "THE TWEET IS: Waiting for love me recuerda tanto a Bath :((\n",
            "THE PROCESSED TWEET IS: ['wait', 'love', 'recuerda', 'tanto', 'bath', ':(']\n",
            "0\t0.52592922\tb'wait love recuerda tanto bath :('\n",
            "THE TWEET IS: that's kind of a dumb statement because who LIKES moodswings?? it's like making a tweet saying \"I hate terminal diseases :(\" like wow unique\n",
            "THE PROCESSED TWEET IS: [\"that'\", 'kind', 'dumb', 'statement', 'like', 'moodsw', 'like', 'make', 'tweet', 'say', 'hate', 'termin', 'diseas', ':(', 'like', 'wow', 'uniqu']\n",
            "0\t0.50029771\tb\"that' kind dumb statement like moodsw like make tweet say hate termin diseas :( like wow uniqu\"\n",
            "THE TWEET IS: Regret and regret :((\n",
            "THE PROCESSED TWEET IS: ['regret', 'regret', ':(']\n",
            "0\t0.50022316\tb'regret regret :('\n",
            "THE TWEET IS: When fave unfollows :(😩💖 @itisfurny http://t.co/hoVQLoKtIg\n",
            "THE PROCESSED TWEET IS: ['fave', 'unfollow', ':(', '😩', '💖']\n",
            "0\t0.50011587\tb'fave unfollow :(  '\n",
            "THE TWEET IS: CUTIE :((( http://t.co/XLVw7xaYSi\n",
            "THE PROCESSED TWEET IS: ['cuti', ':(']\n",
            "0\t0.50000001\tb'cuti :('\n",
            "THE TWEET IS: worried :-(((((((((((\n",
            "THE PROCESSED TWEET IS: ['worri', ':-(']\n",
            "0\t0.50000001\tb'worri :-('\n",
            "THE TWEET IS: @MsMeghanMakeup hope you're having fun at vidcon!! rlly bummed i couldnt go :(( love you sunshine 💛💛💛💛\n",
            "THE PROCESSED TWEET IS: ['hope', 'fun', 'vidcon', 'rlli', 'bum', 'couldnt', 'go', ':(', 'love', 'sunshin', '💛', '💛', '💛']\n",
            "0\t0.52319493\tb'hope fun vidcon rlli bum couldnt go :( love sunshin   '\n",
            "THE TWEET IS: FOLLOW ME PLS!:( @tommophoebe\n",
            "THE PROCESSED TWEET IS: ['follow', 'pl', ':(']\n",
            "0\t0.51623665\tb'follow pl :('\n",
            "THE TWEET IS: my beloved grandmother : ( https://t.co/wt4oXq5xCf\n",
            "THE PROCESSED TWEET IS: ['belov', 'grandmoth']\n",
            "0\t0.50000001\tb'belov grandmoth'\n",
            "THE TWEET IS: Babeeeee :((( you're so demn hotaisndonwyvauwjoqhsjsnaihsuswtf https://t.co/kWwv5Grny7\n",
            "THE PROCESSED TWEET IS: ['babee', ':(', 'demn', 'hotaisndonwyvauwjoqhsjsnaihsuswtf']\n",
            "0\t0.50000001\tb'babee :( demn hotaisndonwyvauwjoqhsjsnaihsuswtf'\n",
            "THE TWEET IS: No words can explain the way i missing you :(((\n",
            "THE PROCESSED TWEET IS: ['word', 'explain', 'way', 'miss', ':(']\n",
            "0\t0.50048501\tb'word explain way miss :('\n",
            "THE TWEET IS: seolhyun isnt in these first eps because shes filming a drama :((\n",
            "THE PROCESSED TWEET IS: ['seolhyun', 'isnt', 'first', 'ep', 'she', 'film', 'drama', ':(']\n",
            "0\t0.50200399\tb'seolhyun isnt first ep she film drama :('\n",
            "THE TWEET IS: Sr. Financial Analyst - Expedia, Inc.: (#Bellevue, WA) http://t.co/ktknMhvwCI #Finance #ExpediaJobs #Job #Jobs #Hiring\n",
            "THE PROCESSED TWEET IS: ['sr', 'financi', 'analyst', 'expedia', 'inc', 'bellevu', 'wa', 'financ', 'expediajob', 'job', 'job', 'hire']\n",
            "0\t0.50000001\tb'sr financi analyst expedia inc bellevu wa financ expediajob job job hire'\n",
            "THE TWEET IS: Overly attached. :((((\n",
            "THE PROCESSED TWEET IS: ['overli', 'attach', ':(']\n",
            "0\t0.50000001\tb'overli attach :('\n",
            "THE TWEET IS: @ShellyBarker123 what's wrong?:((\n",
            "THE PROCESSED TWEET IS: [\"what'\", 'wrong', ':(']\n",
            "0\t0.50000001\tb\"what' wrong :(\"\n",
            "THE TWEET IS: I hit my thumb :(.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-57-289099189.py:9: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE PROCESSED TWEET IS: ['hit', 'thumb', ':(']\n",
            "0\t0.50110710\tb'hit thumb :('\n",
            "THE TWEET IS: Last day on the beach :-(:-(:-(.... http://t.co/oqo9F7FQs9\n",
            "THE PROCESSED TWEET IS: ['last', 'day', 'beach', ':-(', ':-(', ':-(', '...']\n",
            "0\t0.50635527\tb'last day beach :-( :-( :-( ...'\n",
            "THE TWEET IS: @ranaPTX_ BUT WHO WILL LEAD US WHEN YOU'RE GONE :((\n",
            "THE PROCESSED TWEET IS: ['lead', 'us', 'gone', ':(']\n",
            "0\t0.50561261\tb'lead us gone :('\n",
            "THE TWEET IS: @dongvvoo1122 😀🔫 r u sure u want that b r u h we barely survive w them in tank tops :((\n",
            "THE PROCESSED TWEET IS: ['😀', '🔫', 'r', 'u', 'sure', 'u', 'want', 'b', 'r', 'u', 'h', 'bare', 'surviv', 'w', 'tank', 'top', ':(']\n",
            "0\t0.50486844\tb'  r u sure u want b r u h bare surviv w tank top :('\n",
            "THE TWEET IS: so thirsty :((\n",
            "THE PROCESSED TWEET IS: ['thirsti', ':(']\n",
            "0\t0.50000001\tb'thirsti :('\n",
            "THE TWEET IS: where's all the jaebum baby pictures :((\n",
            "THE PROCESSED TWEET IS: [\"where'\", 'jaebum', 'babi', 'pictur', ':(']\n",
            "0\t0.50012445\tb\"where' jaebum babi pictur :(\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to change the tweet below\n",
        "my_tweet = 'This is a ridiculously bright movie. The plot was terrible and I was sad until the ending!'\n",
        "print(process_tweet(my_tweet))\n",
        "y_hat = predict_tweet(my_tweet, freqs, theta)\n",
        "print(y_hat)\n",
        "if y_hat > 0.5:\n",
        "    print('Positive sentiment')\n",
        "else:\n",
        "    print('Negative sentiment')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bo-KwaQSqdNF",
        "outputId": "85e6a160-94ae-4b8b-e1b9-83ee3da4e84b"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ridicul', 'bright', 'movi', 'plot', 'terribl', 'sad', 'end']\n",
            "[0.48723311]\n",
            "Negative sentiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jaxhw77yqjBt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}